# Phase 6 Plan - Advanced Features & Production-Grade Architecture

**Status**: âœ… **ARCHITECTURALLY REFINED** (Ready for Implementation)
**Version**: 2.0 (Comprehensive Architecture Refinement)
**Target Start**: After Phase 5 completion
**Focus**: Advanced runtime features with industrial-grade precision and observability

---

## ðŸ“‹ Executive Summary

Phase 6 delivers **advanced runtime features** with **explicit architectural precision** enabling enterprise-grade migration management. This refined plan incorporates 8 architectural recommendations for production-grade quality.

### Key Improvements Over Phase 6 v1.0
- âœ… Explicit hook execution model (strategy, error handling, timeouts)
- âœ… Type-safe hook contexts with phase-specific data
- âœ… Three-category event system (Lifecycle, State, Alert)
- âœ… Published risk scoring formula with transparent weights
- âœ… Downtime predictions with confidence bounds
- âœ… Rule conflict resolution mechanism
- âœ… Full observability infrastructure (tracing, correlation IDs, circuit breakers)
- âœ… Service level objectives (SLOs) with monitoring

---

## ðŸŽ¯ Phase 6 Objectives (Refined)

1. **Expanded Hook System** - Explicit execution semantics, type safety, observability
2. **Rule Library System** - Conflict resolution, versioning, composition patterns
3. **Performance Profiling** - Observable overhead, confidence tracking, adaptive profiling
4. **Advanced Risk Assessment** - Transparent formula, confidence bounds, calibration
5. **Production Features** - SLO-aware, observable, failure-resilient

---

## ðŸ“Š Detailed Deliverables

### 1. Enhanced Hook System (1,800 lines)

#### 1.1 Hook Event Categorization (Refined)

**Three Categories with Distinct Semantics**:

```python
from enum import Enum
from dataclasses import dataclass
from typing import Generic, TypeVar, Any
from datetime import datetime
from uuid import UUID, uuid4
import logging

logger = logging.getLogger(__name__)

# Category 1: LIFECYCLE EVENTS
# Fired before/after specific operations
class HookPhase(Enum):
    """Lifecycle events - operation boundaries"""
    BEFORE_ANALYZE_SCHEMA = "before_analyze_schema"
    AFTER_ANALYZE_SCHEMA = "after_analyze_schema"
    BEFORE_DIFF_SCHEMAS = "before_diff_schemas"
    AFTER_DIFF_SCHEMAS = "after_diff_schemas"
    BEFORE_PLAN_MIGRATION = "before_plan_migration"
    AFTER_PLAN_MIGRATION = "after_plan_migration"
    BEFORE_DRY_RUN = "before_dry_run"
    AFTER_DRY_RUN = "after_dry_run"
    BEFORE_EXECUTE = "before_execute"
    AFTER_EXECUTE = "after_execute"
    BEFORE_VALIDATE = "before_validate"
    AFTER_VALIDATE = "after_validate"
    BEFORE_ROLLBACK = "before_rollback"
    AFTER_ROLLBACK = "after_rollback"


# Category 2: STATE EVENTS
# Fired when migration enters/leaves a state
class HookEvent(Enum):
    """State change events - observable state transitions"""
    MIGRATION_STARTED = "migration_started"
    MIGRATION_PAUSED = "migration_paused"
    MIGRATION_RESUMED = "migration_resumed"
    MIGRATION_COMPLETED = "migration_completed"
    MIGRATION_FAILED = "migration_failed"
    MIGRATION_ROLLED_BACK = "migration_rolled_back"
    MIGRATION_CANCELLED = "migration_cancelled"


# Category 3: ALERT EVENTS
# Fired when metrics cross thresholds
class HookAlert(Enum):
    """Threshold alerts - reactive to metric crossings"""
    DATA_ANOMALY_DETECTED = "data_anomaly_detected"
    LOCK_TIMEOUT_EXCEEDED = "lock_timeout_exceeded"
    PERFORMANCE_DEGRADED = "performance_degraded"
    MEMORY_THRESHOLD_EXCEEDED = "memory_threshold_exceeded"
    LONG_TRANSACTION_DETECTED = "long_transaction_detected"
    CONNECTION_POOL_EXHAUSTED = "connection_pool_exhausted"
```

**Benefit**: Clear semantic separation. Different event types require different handlers.

---

#### 1.2 Hook Execution Model (NEW - CRITICAL)

```python
from enum import Enum

class HookExecutionStrategy(Enum):
    """Defines how hooks execute within a phase"""
    SEQUENTIAL = "sequential"                    # One by one, in priority order
    PARALLEL = "parallel"                        # All simultaneously via asyncio.gather()
    PARALLEL_WITH_DEPS = "parallel_with_deps"   # DAG execution respecting dependencies


class HookErrorStrategy(Enum):
    """What happens when a hook fails"""
    FAIL_FAST = "fail_fast"           # Stop execution, fail migration
    FAIL_SAFE = "fail_safe"           # Log error, continue migration
    RETRY = "retry"                   # Retry with exponential backoff
    ALERT_ONLY = "alert_only"         # Alert but continue


class HookContextMutationPolicy(Enum):
    """Whether downstream hooks can see upstream modifications"""
    IMMUTABLE = "immutable"            # Context is read-only
    MUTABLE = "mutable"               # Hooks can modify for downstream
    COPY_ON_WRITE = "copy_on_write"   # Each hook gets modified copy


@dataclass
class RetryConfig:
    """Retry strategy for RETRY error handling"""
    max_attempts: int = 3
    initial_delay_ms: int = 100
    max_delay_ms: int = 30000
    backoff_multiplier: float = 2.0


@dataclass
class HookPhaseConfig:
    """Configuration for hook execution in a specific phase"""
    phase: HookPhase | HookEvent | HookAlert
    execution_strategy: HookExecutionStrategy = HookExecutionStrategy.SEQUENTIAL
    error_strategy: HookErrorStrategy = HookErrorStrategy.FAIL_FAST
    context_mutation_policy: HookContextMutationPolicy = HookContextMutationPolicy.IMMUTABLE
    timeout_per_hook_ms: int = 30000           # 30 seconds per hook
    timeout_per_phase_ms: int = 300000         # 5 minutes per phase
    max_parallel_hooks: int = 4                # Limit concurrent execution
    retry_config: RetryConfig | None = None    # For RETRY strategy
    circuit_breaker_enabled: bool = True       # Prevent cascading failures


# Usage Example:
HOOK_EXECUTION_CONFIG = {
    HookPhase.BEFORE_ANALYZE_SCHEMA: HookPhaseConfig(
        phase=HookPhase.BEFORE_ANALYZE_SCHEMA,
        execution_strategy=HookExecutionStrategy.PARALLEL,
        error_strategy=HookErrorStrategy.FAIL_SAFE,
        timeout_per_hook_ms=5000,
    ),
    HookPhase.AFTER_EXECUTE: HookPhaseConfig(
        phase=HookPhase.AFTER_EXECUTE,
        execution_strategy=HookExecutionStrategy.PARALLEL_WITH_DEPS,
        error_strategy=HookErrorStrategy.RETRY,
        retry_config=RetryConfig(max_attempts=3),
        timeout_per_hook_ms=30000,
    ),
    HookEvent.MIGRATION_FAILED: HookPhaseConfig(
        phase=HookEvent.MIGRATION_FAILED,
        execution_strategy=HookExecutionStrategy.SEQUENTIAL,
        error_strategy=HookErrorStrategy.ALERT_ONLY,
    ),
}
```

**Benefit**: Explicit guarantees. No ambiguity about hook execution.

---

#### 1.3 Type-Safe Hook Contexts (NEW - CRITICAL)

```python
T = TypeVar('T')

@dataclass
class SchemaAnalysisContext:
    """Context available in before/after_analyze_schema hooks"""
    source_schema: Schema
    target_schema: Schema
    analysis_time_ms: int
    tables_analyzed: int
    columns_analyzed: int
    metadata: dict[str, Any]


@dataclass
class SchemaDiffContext:
    """Context available in before/after_diff_schemas hooks"""
    source_schema: Schema
    target_schema: Schema
    differences: list[SchemaDifference]
    diff_time_ms: int
    breaking_changes: list[str]
    safe_changes: list[str]
    metadata: dict[str, Any]


@dataclass
class MigrationPlanContext:
    """Context available in before/after_plan_migration hooks"""
    migration_steps: list[MigrationStep]
    estimated_duration_ms: int
    estimated_downtime_ms: int
    risk_assessment: RiskAssessment
    affected_tables: list[str]
    metadata: dict[str, Any]


@dataclass
class ExecutionContext:
    """Context available during before/after_execute"""
    current_step: MigrationStep
    steps_completed: int
    total_steps: int
    elapsed_time_ms: int
    rows_affected: int
    current_connections: int
    metadata: dict[str, Any]


@dataclass
class RollbackContext:
    """Context available during before/after_rollback"""
    rollback_reason: str
    steps_to_rollback: list[MigrationStep]
    original_error: Exception | None
    metadata: dict[str, Any]


class HookContext(Generic[T]):
    """Type-safe hook context with phase-specific information"""

    def __init__(
        self,
        phase: HookPhase | HookEvent | HookAlert,
        data: T,
        execution_id: UUID,
        hook_id: str,
    ):
        self.phase = phase
        self.data: T = data                      # Type-safe data
        self.execution_id = execution_id          # Correlation ID for tracing
        self.hook_id = hook_id
        self.timestamp = datetime.utcnow()
        self.parent_execution_id: UUID | None = None  # For nested hooks

    def get_data(self) -> T:
        """Get phase-specific data (type-safe)"""
        return self.data

    def add_metadata(self, key: str, value: Any) -> None:
        """Add metadata for observability"""
        self.data.metadata[key] = value
```

**Benefit**: IDE autocomplete, compile-time checking, self-documenting code.

---

#### 1.4 Hook Registry with Execution Strategy

```python
class HookRegistry:
    """Manage hook registration and execution"""

    def __init__(self, execution_config: dict[Any, HookPhaseConfig] | None = None):
        self.hooks: dict[str, list[Hook]] = {}  # phase/event/alert -> hooks
        self.execution_config = execution_config or {}
        self.circuit_breakers: dict[str, CircuitBreaker] = {}
        self.execution_log: list[HookExecutionEvent] = []

    async def trigger(
        self,
        phase: HookPhase | HookEvent | HookAlert,
        context: HookContext[T],
    ) -> HookExecutionResult:
        """Trigger hooks for a phase/event/alert"""

        config = self.execution_config.get(phase, HookPhaseConfig(phase=phase))
        hooks = self.hooks.get(phase.value, [])

        if not hooks:
            return HookExecutionResult(phase=phase.value, hooks_executed=0)

        # Execute according to strategy
        if config.execution_strategy == HookExecutionStrategy.SEQUENTIAL:
            return await self._execute_sequential(context, hooks, config)
        elif config.execution_strategy == HookExecutionStrategy.PARALLEL:
            return await self._execute_parallel(context, hooks, config)
        elif config.execution_strategy == HookExecutionStrategy.PARALLEL_WITH_DEPS:
            return await self._execute_dag(context, hooks, config)

    async def _execute_sequential(
        self,
        context: HookContext[T],
        hooks: list[Hook],
        config: HookPhaseConfig,
    ) -> HookExecutionResult:
        """Execute hooks one-by-one"""
        results = []

        for hook in sorted(hooks, key=lambda h: h.priority):
            try:
                result = await self._execute_hook_with_timeout(
                    hook, context, config
                )
                results.append(result)

                # Check if we should fail fast
                if result.failed and config.error_strategy == HookErrorStrategy.FAIL_FAST:
                    raise HookExecutionError(
                        f"Hook {hook.name} failed: {result.error}"
                    )

            except Exception as e:
                if config.error_strategy == HookErrorStrategy.FAIL_SAFE:
                    logger.error(f"Hook {hook.name} failed: {e}")
                elif config.error_strategy == HookErrorStrategy.RETRY:
                    result = await self._retry_hook(hook, context, config)
                    results.append(result)
                else:
                    raise

        return HookExecutionResult(
            phase=config.phase.value,
            hooks_executed=len(results),
            results=results,
        )

    async def _execute_parallel(
        self,
        context: HookContext[T],
        hooks: list[Hook],
        config: HookPhaseConfig,
    ) -> HookExecutionResult:
        """Execute hooks in parallel"""
        import asyncio

        # Limit parallelism
        semaphore = asyncio.Semaphore(config.max_parallel_hooks)

        async def execute_with_semaphore(hook: Hook) -> HookExecutionEvent:
            async with semaphore:
                return await self._execute_hook_with_timeout(hook, context, config)

        tasks = [execute_with_semaphore(hook) for hook in hooks]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        failed = [r for r in results if isinstance(r, Exception) or r.failed]
        if failed and config.error_strategy == HookErrorStrategy.FAIL_FAST:
            raise HookExecutionError(
                f"{len(failed)} hooks failed in parallel execution"
            )

        return HookExecutionResult(
            phase=config.phase.value,
            hooks_executed=len(results),
            results=results,
        )

    async def _execute_hook_with_timeout(
        self,
        hook: Hook,
        context: HookContext[T],
        config: HookPhaseConfig,
    ) -> HookExecutionEvent:
        """Execute single hook with timeout enforcement"""
        import asyncio

        start = datetime.utcnow()
        circuit_breaker = self.circuit_breakers.get(hook.id)

        if circuit_breaker and circuit_breaker.is_open:
            return HookExecutionEvent(
                execution_id=context.execution_id,
                hook_id=hook.id,
                phase=config.phase.value,
                status=HookExecutionStatus.SKIPPED,
                reason="Circuit breaker open",
                duration_ms=0,
            )

        try:
            result = await asyncio.wait_for(
                hook.execute(context),
                timeout=config.timeout_per_hook_ms / 1000,
            )
            duration = (datetime.utcnow() - start).total_seconds() * 1000

            event = HookExecutionEvent(
                execution_id=context.execution_id,
                hook_id=hook.id,
                phase=config.phase.value,
                status=HookExecutionStatus.COMPLETED,
                duration_ms=int(duration),
                rows_affected=result.rows_affected,
                stats=result.stats,
            )

            if circuit_breaker:
                circuit_breaker.record_success()

            return event

        except asyncio.TimeoutError:
            if circuit_breaker:
                circuit_breaker.record_failure()
            return HookExecutionEvent(
                execution_id=context.execution_id,
                hook_id=hook.id,
                phase=config.phase.value,
                status=HookExecutionStatus.TIMEOUT,
                reason=f"Exceeded {config.timeout_per_hook_ms}ms timeout",
                duration_ms=config.timeout_per_hook_ms,
            )
        except Exception as e:
            if circuit_breaker:
                circuit_breaker.record_failure()
            return HookExecutionEvent(
                execution_id=context.execution_id,
                hook_id=hook.id,
                phase=config.phase.value,
                status=HookExecutionStatus.FAILED,
                error=str(e),
                duration_ms=int((datetime.utcnow() - start).total_seconds() * 1000),
            )
```

**Benefit**: Deterministic execution with clear semantics.

---

#### 1.5 Observability & Tracing Infrastructure

```python
from dataclasses import dataclass
from enum import Enum


class HookExecutionStatus(Enum):
    """Status of hook execution"""
    PENDING = "pending"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"
    SKIPPED = "skipped"


@dataclass
class HookExecutionEvent:
    """Record of a single hook execution"""
    execution_id: UUID              # Trace correlation ID
    hook_id: str
    phase: str
    status: HookExecutionStatus
    duration_ms: int
    rows_affected: int = 0
    error: str | None = None
    reason: str | None = None
    stats: dict[str, Any] | None = None
    timestamp: datetime = field(default_factory=datetime.utcnow)


@dataclass
class HookExecutionResult:
    """Result of executing all hooks in a phase"""
    phase: str
    hooks_executed: int
    results: list[HookExecutionEvent] | None = None
    total_duration_ms: int = 0
    failed_count: int = 0
    timeout_count: int = 0


class HookExecutionTracer:
    """Track and trace hook execution for debugging"""

    def __init__(self):
        self.execution_log: list[HookExecutionEvent] = []
        self.execution_graphs: dict[UUID, ExecutionDAG] = {}

    def record_execution(self, event: HookExecutionEvent) -> None:
        """Record hook execution event"""
        self.execution_log.append(event)
        logger.info(
            f"Hook {event.hook_id} in {event.phase}: {event.status.value} "
            f"({event.duration_ms}ms)"
        )

    def get_execution_log(
        self,
        execution_id: UUID | None = None,
        phase: str | None = None,
    ) -> list[HookExecutionEvent]:
        """Get execution log with optional filtering"""
        log = self.execution_log

        if execution_id:
            log = [e for e in log if e.execution_id == execution_id]

        if phase:
            log = [e for e in log if e.phase == phase]

        return log

    def get_execution_dag(self, execution_id: UUID) -> ExecutionDAG | None:
        """Get execution DAG showing hook dependencies"""
        return self.execution_graphs.get(execution_id)

    def get_performance_trace(
        self,
        execution_id: UUID,
    ) -> PerformanceTrace:
        """Get detailed performance trace"""
        events = self.get_execution_log(execution_id=execution_id)

        return PerformanceTrace(
            execution_id=execution_id,
            total_duration_ms=sum(e.duration_ms for e in events),
            hook_events=events,
            critical_path=self._compute_critical_path(events),
        )

    def _compute_critical_path(
        self,
        events: list[HookExecutionEvent],
    ) -> list[str]:
        """Compute critical path (longest chain of dependent hooks)"""
        # Implementation uses DAG analysis
        pass


class CircuitBreaker:
    """Prevent cascading failures from failing hooks"""

    def __init__(
        self,
        hook_id: str,
        failure_threshold: int = 5,
        recovery_timeout_ms: int = 60000,
    ):
        self.hook_id = hook_id
        self.failure_threshold = failure_threshold
        self.recovery_timeout_ms = recovery_timeout_ms
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitBreakerState.CLOSED  # CLOSED | OPEN | HALF_OPEN

    @property
    def is_open(self) -> bool:
        """Is circuit breaker open (blocking requests)?"""
        if self.state == CircuitBreakerState.OPEN:
            # Check if recovery timeout has elapsed
            if (
                self.last_failure_time
                and (datetime.utcnow() - self.last_failure_time).total_seconds()
                * 1000
                > self.recovery_timeout_ms
            ):
                self.state = CircuitBreakerState.HALF_OPEN
                self.failure_count = 0
                return False
            return True
        return False

    def record_success(self) -> None:
        """Record successful hook execution"""
        if self.state == CircuitBreakerState.HALF_OPEN:
            self.state = CircuitBreakerState.CLOSED
            self.failure_count = 0

    def record_failure(self) -> None:
        """Record failed hook execution"""
        self.failure_count += 1
        self.last_failure_time = datetime.utcnow()

        if self.failure_count >= self.failure_threshold:
            self.state = CircuitBreakerState.OPEN
            logger.warning(
                f"Circuit breaker opened for hook {self.hook_id} after "
                f"{self.failure_count} failures"
            )


class CircuitBreakerState(Enum):
    """Circuit breaker states"""
    CLOSED = "closed"         # Normal operation
    OPEN = "open"            # Blocking requests
    HALF_OPEN = "half_open"  # Testing recovery
```

**Benefit**: Full observability for debugging and monitoring.

---

#### 1.6 Hook Definition with Priority and Dependencies

```python
from abc import ABC, abstractmethod

class Hook(Generic[T], ABC):
    """Base class for all hooks"""

    def __init__(
        self,
        hook_id: str,
        name: str,
        priority: int = 5,  # 1-10, lower = higher priority
        depends_on: list[str] | None = None,
    ):
        self.id = hook_id
        self.name = name
        self.priority = priority
        self.depends_on = depends_on or []

    @abstractmethod
    async def execute(self, context: HookContext[T]) -> HookResult:
        """Execute hook - must be implemented by subclasses"""
        pass


@dataclass
class HookResult:
    """Result of hook execution"""
    success: bool
    rows_affected: int = 0
    stats: dict[str, Any] | None = None
    error: str | None = None
```

---

**Hook System Deliverables**:
- âœ… `python/confiture/core/hooks/phases.py` (100 lines) - Event categorization
- âœ… `python/confiture/core/hooks/execution.py` (600 lines) - Registry, executor, strategies
- âœ… `python/confiture/core/hooks/context.py` (200 lines) - Type-safe contexts
- âœ… `python/confiture/core/hooks/observability.py` (300 lines) - Tracing, circuit breakers
- âœ… `python/confiture/core/hooks/base.py` (100 lines) - Hook base class
- âœ… Tests: `tests/unit/test_hooks_execution.py` (600 lines)
- âœ… Tests: `tests/integration/test_hooks_observability.py` (400 lines)

**Total Hook System**: 1,800 lines (vs 1,200 in v1.0)

---

### 2. Rule Library System (2,000 lines)

#### 2.1 Rule Versioning with Deprecation Path (NEW)

```python
from dataclasses import dataclass

@dataclass
class RuleVersion:
    """Semantic version for rules"""
    major: int
    minor: int
    patch: int

    def __str__(self) -> str:
        return f"{self.major}.{self.minor}.{self.patch}"

    def is_compatible_with(self, other: "RuleVersion") -> bool:
        """Check if compatible (major version must match)"""
        return self.major == other.major

    def __le__(self, other: "RuleVersion") -> bool:
        return (self.major, self.minor, self.patch) <= (
            other.major,
            other.minor,
            other.patch,
        )

    def __ge__(self, other: "RuleVersion") -> bool:
        return (self.major, self.minor, self.patch) >= (
            other.major,
            other.minor,
            other.patch,
        )


@dataclass
class Rule:
    """Individual linting rule with versioning"""
    rule_id: str
    name: str
    description: str
    version: RuleVersion
    deprecated_in: RuleVersion | None = None
    removed_in: RuleVersion | None = None
    migration_path: str | None = None         # Docs URL or replacement rule ID
    severity: LintSeverity = LintSeverity.WARNING
    enabled_by_default: bool = True

    def is_deprecated(self, target_version: RuleVersion | None = None) -> bool:
        """Check if rule is deprecated"""
        if not self.deprecated_in:
            return False
        if target_version:
            return self.deprecated_in <= target_version
        return True

    def is_removed(self, target_version: RuleVersion | None = None) -> bool:
        """Check if rule is removed"""
        if not self.removed_in:
            return False
        if target_version:
            return self.removed_in <= target_version
        return True


class RuleVersionManager:
    """Manage rule versions and compatibility"""

    def __init__(self, rules: list[Rule]):
        self.rules = {r.rule_id: r for r in rules}

    def get_rule(
        self,
        rule_id: str,
        target_version: RuleVersion | None = None,
    ) -> Rule | None:
        """Get rule compatible with target version"""
        rule = self.rules.get(rule_id)
        if not rule:
            return None

        if rule.is_removed(target_version):
            raise RuleRemovedError(
                f"Rule {rule_id} was removed in {rule.removed_in}. "
                f"Migration path: {rule.migration_path}"
            )

        if rule.is_deprecated(target_version):
            logger.warning(
                f"Rule {rule_id} is deprecated since {rule.deprecated_in}. "
                f"Migration path: {rule.migration_path}"
            )

        return rule

    def validate_compatibility(
        self,
        library_version: RuleVersion,
        min_rule_version: RuleVersion,
    ) -> list[str]:
        """Check if all rules are compatible with version"""
        incompatible = []
        for rule in self.rules.values():
            if not rule.version.is_compatible_with(min_rule_version):
                incompatible.append(rule.rule_id)
        return incompatible
```

---

#### 2.2 Rule Conflict Detection & Resolution (NEW - CRITICAL)

```python
from dataclasses import dataclass
from enum import Enum


class ConflictResolution(Enum):
    """How to handle rule conflicts"""
    ERROR = "error"              # Raise exception
    WARN = "warn"                # Log warning, continue
    PREFER_FIRST = "prefer_first"   # Use first added library's rule
    PREFER_LAST = "prefer_last"     # Use last added library's rule


class ConflictType(Enum):
    """Type of conflict between rules"""
    DUPLICATE = "duplicate"               # Same rule ID
    INCOMPATIBLE = "incompatible"        # Conflicting requirements
    OVERLAPPING = "overlapping"          # Similar functionality


@dataclass
class RuleConflict:
    """Represents a conflict between rules"""
    rule_id: str
    library_a: str
    library_b: str
    conflict_type: ConflictType
    severity: LintSeverity
    description: str
    suggested_resolution: str


class RuleLibrary:
    """Collection of related rules"""

    def __init__(
        self,
        name: str,
        version: RuleVersion,
        rules: list[Rule],
        tags: list[str] | None = None,
    ):
        self.name = name
        self.version = version
        self.rules = {r.rule_id: r for r in rules}
        self.tags = tags or []

    def get_rules(self) -> list[Rule]:
        """Get all rules in this library"""
        return list(self.rules.values())


class RuleLibraryComposer:
    """Compose multiple libraries with explicit conflict handling"""

    def __init__(self):
        self.libraries: list[RuleLibrary] = []
        self.overrides: dict[str, Rule] = {}
        self.disabled_rules: set[str] = set()
        self.conflict_log: list[RuleConflict] = []

    def add_library(
        self,
        library: RuleLibrary,
        on_conflict: ConflictResolution = ConflictResolution.ERROR,
    ) -> "RuleLibraryComposer":
        """Add library with conflict handling"""
        conflicts = self._detect_conflicts(library)

        if conflicts:
            self.conflict_log.extend(conflicts)

            if on_conflict == ConflictResolution.ERROR:
                raise RuleConflictError(
                    f"Found {len(conflicts)} rule conflicts in {library.name}"
                )
            elif on_conflict == ConflictResolution.WARN:
                for conflict in conflicts:
                    logger.warning(
                        f"Rule conflict: {conflict.rule_id} in {conflict.library_a} "
                        f"vs {conflict.library_b}. {conflict.suggested_resolution}"
                    )

        self.libraries.append(library)
        return self

    def override_rule(self, rule_id: str, new_rule: Rule) -> "RuleLibraryComposer":
        """Override a specific rule"""
        self.overrides[rule_id] = new_rule
        logger.info(f"Overridden rule {rule_id}")
        return self

    def disable_rule(self, rule_id: str) -> "RuleLibraryComposer":
        """Disable a rule from any library"""
        self.disabled_rules.add(rule_id)
        logger.info(f"Disabled rule {rule_id}")
        return self

    def build(self) -> ComposedRuleSet:
        """Build final rule set with conflicts resolved"""
        all_rules = {}

        for library in self.libraries:
            for rule_id, rule in library.rules.items():
                if rule_id in self.disabled_rules:
                    continue
                all_rules[rule_id] = rule

        # Apply overrides
        all_rules.update(self.overrides)

        # Create audit trail
        return ComposedRuleSet(
            rules=list(all_rules.values()),
            libraries=[lib.name for lib in self.libraries],
            disabled_rules=list(self.disabled_rules),
            overridden_rules=list(self.overrides.keys()),
            conflicts=self.conflict_log,
        )

    def _detect_conflicts(self, new_library: RuleLibrary) -> list[RuleConflict]:
        """Detect conflicts with existing libraries"""
        conflicts = []
        new_rule_ids = set(new_library.rules.keys())

        for existing_library in self.libraries:
            for existing_rule_id in existing_library.rules.keys():
                if existing_rule_id in new_rule_ids:
                    conflicts.append(
                        RuleConflict(
                            rule_id=existing_rule_id,
                            library_a=existing_library.name,
                            library_b=new_library.name,
                            conflict_type=ConflictType.DUPLICATE,
                            severity=LintSeverity.WARNING,
                            description=f"Rule {existing_rule_id} exists in both libraries",
                            suggested_resolution=(
                                f"Use override_rule() to select preferred version"
                            ),
                        )
                    )

        return conflicts


@dataclass
class ComposedRuleSet:
    """Result of composing multiple rule libraries"""
    rules: list[Rule]
    libraries: list[str]           # Which libraries were composed
    disabled_rules: list[str]      # Which rules were disabled
    overridden_rules: list[str]    # Which rules were overridden
    conflicts: list[RuleConflict]

    def get_audit_trail(self) -> str:
        """Get human-readable audit trail"""
        lines = [
            f"Libraries: {', '.join(self.libraries)}",
            f"Total rules: {len(self.rules)}",
            f"Disabled: {len(self.disabled_rules)} ({', '.join(self.disabled_rules)})",
            f"Overridden: {len(self.overridden_rules)}",
            f"Conflicts: {len(self.conflicts)}",
        ]
        return "\n".join(lines)
```

**Benefit**: Transparent conflict handling. Audit trail shows exactly what's active.

---

#### 2.3 Rule Libraries (HIPAA, SOX, GDPR, PCI-DSS, General)

**Each library implements**:

```python
class HIPAALibrary(RuleLibrary):
    """HIPAA compliance rule library (15 rules)"""

    def __init__(self):
        super().__init__(
            name="HIPAA",
            version=RuleVersion(major=1, minor=0, patch=0),
            rules=[
                Rule(
                    rule_id="hipaa_001",
                    name="encrypt_phi",
                    description="All PII/PHI columns must be encrypted",
                    version=RuleVersion(1, 0, 0),
                    severity=LintSeverity.ERROR,
                ),
                Rule(
                    rule_id="hipaa_002",
                    name="audit_log_retention",
                    description="Maintain audit logs for min 6 years",
                    version=RuleVersion(1, 0, 0),
                    severity=LintSeverity.ERROR,
                ),
                # 13 more rules...
            ],
            tags=["healthcare", "compliance", "phi"],
        )


class SOXLibrary(RuleLibrary):
    """SOX compliance rule library (12 rules)"""
    # Similar structure


class GDPRLibrary(RuleLibrary):
    """GDPR compliance rule library (18 rules)"""
    # Similar structure


class PCI_DSSLibrary(RuleLibrary):
    """PCI-DSS compliance rule library (10 rules)"""
    # Similar structure


class GeneralLibrary(RuleLibrary):
    """General best practices rule library (20+ rules)"""
    # Similar structure
```

---

**Rule Library Deliverables**:
- âœ… `python/confiture/core/linting/versioning.py` (150 lines)
- âœ… `python/confiture/core/linting/composer.py` (400 lines)
- âœ… `python/confiture/core/linting/libraries/hipaa.py` (200 lines)
- âœ… `python/confiture/core/linting/libraries/sox.py` (180 lines)
- âœ… `python/confiture/core/linting/libraries/gdpr.py` (250 lines)
- âœ… `python/confiture/core/linting/libraries/pci_dss.py` (150 lines)
- âœ… `python/confiture/core/linting/libraries/general.py` (300 lines)
- âœ… `python/confiture/core/linting/registry.py` (150 lines)
- âœ… Tests: `tests/unit/test_rule_libraries.py` (600 lines)

**Total Rule Library System**: 2,000 lines (vs 1,500 in v1.0)

---

### 3. Performance Profiling System (1,400 lines)

#### 3.1 Observable Profiling Overhead (NEW - CRITICAL)

```python
from dataclasses import dataclass

@dataclass
class ProfilingMetadata:
    """Metadata about profiling run"""
    total_queries: int
    profiled_queries: int                  # Might be sampled
    sampling_rate: float                   # 0.0-1.0
    profiling_overhead_ms: int
    query_time_without_profiling_ms: int
    profiling_overhead_percent: float
    confidence_level: float                # 0.0-1.0, lower if sampled
    is_deterministic: bool                 # False if sampling
    skipped_analysis_reasons: list[str]   # Why analysis was skipped


@dataclass
class QueryProfile:
    """Individual query profile"""
    query_hash: str
    query_text: str
    execution_count: int
    total_duration_ms: int
    avg_duration_ms: float
    min_duration_ms: int
    max_duration_ms: int
    has_sequential_scans: bool
    has_sorts: bool
    estimated_rows: int
    actual_rows: int
    plan_quality_estimate: float            # 0.0-1.0


class QueryProfiler:
    """Profile query performance with overhead tracking"""

    def __init__(
        self,
        target_overhead_percent: float = 5.0,
        sampling_rate: float = 1.0,  # 1.0 = profile all
    ):
        self.target_overhead_percent = target_overhead_percent
        self.sampling_rate = sampling_rate
        self.profiles: dict[str, QueryProfile] = {}
        self.profiling_metadata: dict[str, ProfilingMetadata] = {}

    async def profile_query(
        self,
        query: str,
        params: tuple = (),
        connection: AsyncConnection | None = None,
    ) -> tuple[QueryProfile, ProfilingMetadata]:
        """Profile query with overhead tracking"""
        import random
        import hashlib
        import time

        query_hash = hashlib.sha256(query.encode()).hexdigest()[:8]

        if random.random() > self.sampling_rate:
            # Sampling: skip profiling
            start = time.perf_counter()
            result = await connection.cursor().execute(query, params)
            duration = time.perf_counter() - start

            return None, ProfilingMetadata(
                total_queries=1,
                profiled_queries=0,
                sampling_rate=self.sampling_rate,
                profiling_overhead_ms=0,
                query_time_without_profiling_ms=int(duration * 1000),
                profiling_overhead_percent=0.0,
                confidence_level=self.sampling_rate,
                is_deterministic=self.sampling_rate == 1.0,
                skipped_analysis_reasons=["Sampling"],
            )

        # Profile the query
        profiling_start = time.perf_counter()

        # Execute query
        query_start = time.perf_counter()
        cursor = await connection.cursor()
        cursor.execute(query, params)
        result = cursor.fetchall()
        query_duration = time.perf_counter() - query_start

        # Get query plan
        plan_start = time.perf_counter()
        try:
            cursor.execute(f"EXPLAIN ANALYZE {query}", params)
            plan = cursor.fetchall()
        except Exception as e:
            logger.warning(f"Could not analyze query plan: {e}")
            plan = None

        plan_duration = time.perf_counter() - plan_start

        profiling_overhead = time.perf_counter() - profiling_start - query_duration
        profiling_overhead_percent = (profiling_overhead / query_duration) * 100

        skipped_reasons = []
        # Skip expensive analysis if overhead exceeds target
        if profiling_overhead_percent > self.target_overhead_percent:
            logger.warning(
                f"Profiling overhead {profiling_overhead_percent:.1f}% exceeds "
                f"target {self.target_overhead_percent}%. Skipping detailed analysis."
            )
            skipped_reasons.append(
                f"Overhead {profiling_overhead_percent:.1f}% > "
                f"target {self.target_overhead_percent}%"
            )
            plan = None

        profile = QueryProfile(
            query_hash=query_hash,
            query_text=query,
            execution_count=1,
            total_duration_ms=int(query_duration * 1000),
            avg_duration_ms=query_duration * 1000,
            min_duration_ms=int(query_duration * 1000),
            max_duration_ms=int(query_duration * 1000),
            has_sequential_scans="Seq Scan" in (plan or ""),
            has_sorts="Sort" in (plan or ""),
            estimated_rows=0,  # Parse from plan
            actual_rows=len(result),
            plan_quality_estimate=1.0,
        )

        metadata = ProfilingMetadata(
            total_queries=1,
            profiled_queries=1,
            sampling_rate=self.sampling_rate,
            profiling_overhead_ms=int(profiling_overhead * 1000),
            query_time_without_profiling_ms=int(query_duration * 1000),
            profiling_overhead_percent=profiling_overhead_percent,
            confidence_level=self.sampling_rate,
            is_deterministic=self.sampling_rate == 1.0,
            skipped_analysis_reasons=skipped_reasons,
        )

        self.profiles[query_hash] = profile
        self.profiling_metadata[query_hash] = metadata

        return profile, metadata
```

**Benefit**: Know exactly what profiling costs.

---

#### 3.2 Statistical Baseline Management (NEW - CRITICAL)

```python
from dataclasses import dataclass, field
import statistics

@dataclass
class PerformanceBaseline:
    """Performance baseline with statistical confidence"""
    operation_id: str                          # Query hash or migration name
    environment: str                           # "local", "staging", "production"
    baseline_duration_ms: int                  # Mean
    confidence_interval: tuple[int, int]       # (lower, upper) - 95% CI
    sample_count: int                          # How many measurements
    recorded_at: datetime = field(default_factory=datetime.utcnow)
    recorded_by_version: str = ""
    baseline_age_days: int = 0
    confidence_level: float = 0.95             # Statistical confidence


class PerformanceBaselineManager:
    """Manage performance baselines with precision"""

    def __init__(self, storage: BaselineStorage):
        self.storage = storage

    def record_baseline(
        self,
        operation_id: str,
        environment: str,
        measurements: list[int],  # Duration in ms
        version: str,
    ) -> PerformanceBaseline:
        """Record baseline with statistical confidence"""

        # Calculate statistics
        mean = statistics.mean(measurements)
        stdev = (
            statistics.stdev(measurements)
            if len(measurements) > 1
            else 0
        )

        # 95% confidence interval
        confidence_interval = (
            max(0, int(mean - 2 * stdev)),   # Lower bound
            int(mean + 2 * stdev),            # Upper bound
        )

        baseline = PerformanceBaseline(
            operation_id=operation_id,
            environment=environment,
            baseline_duration_ms=int(mean),
            confidence_interval=confidence_interval,
            sample_count=len(measurements),
            recorded_by_version=version,
            confidence_level=0.95,
        )

        self.storage.save(baseline)
        logger.info(
            f"Recorded baseline for {operation_id} in {environment}: "
            f"{baseline.baseline_duration_ms}ms (CI: {confidence_interval})"
        )
        return baseline

    def check_regression(
        self,
        operation_id: str,
        environment: str,
        actual_duration_ms: int,
        regression_threshold_percent: float = 20.0,
    ) -> RegressionResult:
        """Check if actual performance is a regression"""

        baseline = self.storage.get(operation_id, environment)

        if not baseline:
            return RegressionResult(
                is_regression=False,
                reason="NO_BASELINE",
                message=f"No baseline for {operation_id} in {environment}",
                severity=IssueSeverity.INFO,
            )

        # Check baseline age
        baseline_age_days = (datetime.utcnow() - baseline.recorded_at).days
        if baseline_age_days > 30:
            return RegressionResult(
                is_regression=False,
                reason="BASELINE_STALE",
                message=f"Baseline is {baseline_age_days} days old, treating as stale",
                severity=IssueSeverity.WARNING,
            )

        # Check if within confidence interval
        lower, upper = baseline.confidence_interval

        if actual_duration_ms < lower:
            improvement_percent = (
                (baseline.baseline_duration_ms - actual_duration_ms)
                / baseline.baseline_duration_ms
                * 100
            )
            return RegressionResult(
                is_regression=False,
                reason="IMPROVEMENT",
                message=f"Performance improved {improvement_percent:.1f}%: "
                f"{actual_duration_ms}ms vs {baseline.baseline_duration_ms}ms",
                severity=IssueSeverity.INFO,
            )

        if actual_duration_ms > upper:
            percent_increase = (
                (actual_duration_ms - baseline.baseline_duration_ms)
                / baseline.baseline_duration_ms
                * 100
            )

            if percent_increase > regression_threshold_percent:
                return RegressionResult(
                    is_regression=True,
                    reason="REGRESSION",
                    message=f"Performance regressed {percent_increase:.1f}%: "
                    f"{actual_duration_ms}ms vs baseline {baseline.baseline_duration_ms}ms",
                    severity=IssueSeverity.ERROR,
                )

        return RegressionResult(
            is_regression=False,
            reason="OK",
            message=f"Within confidence interval ({lower}-{upper}ms)",
            severity=IssueSeverity.OK,
        )

    def get_evolution(
        self,
        operation_id: str,
        environment: str,
        days: int = 30,
    ) -> list[PerformanceBaseline]:
        """Get baseline evolution over time"""
        return self.storage.get_history(operation_id, environment, days)
```

**Benefit**: Statistical rigor. Know when predictions are reliable.

---

**Performance Profiling Deliverables**:
- âœ… `python/confiture/core/performance/query_profiler.py` (400 lines)
- âœ… `python/confiture/core/performance/baseline_manager.py` (300 lines)
- âœ… `python/confiture/core/performance/recommendations.py` (200 lines)
- âœ… `python/confiture/core/performance/storage.py` (150 lines)
- âœ… Tests: `tests/integration/test_performance_profiling.py` (500 lines)
- âœ… Tests: `tests/integration/test_baseline_management.py` (350 lines)

**Total Performance Profiling**: 1,400 lines (vs 1,000 in v1.0)

---

### 4. Advanced Risk Assessment (1,200 lines)

#### 4.1 Transparent Risk Scoring Formula (NEW - CRITICAL)

```python
from dataclasses import dataclass
from enum import Enum


class RiskLevel(Enum):
    """Risk classification"""
    LOW = 1          # <100ms estimated downtime
    MEDIUM = 2       # 100ms - 1s estimated downtime
    HIGH = 3         # 1s - 10s estimated downtime
    CRITICAL = 4     # >10s estimated downtime


@dataclass
class RiskFactor:
    """Individual risk factor with scoring"""
    name: str
    value: float                    # 0.0-1.0
    unit: str                       # "percent", "seconds", "bytes", etc.
    weight: float                   # Contribution to overall score
    description: str


class RiskScoringFormula:
    """
    EXPLICIT RISK SCORING FORMULA

    This class documents the exact algorithm used to calculate risk scores.
    All weights and thresholds are configurable.
    """

    # Weighting factors (must sum to 1.0)
    WEIGHT_DATA_VOLUME = 0.25
    WEIGHT_LOCK_TIME = 0.35
    WEIGHT_DEPENDENCIES = 0.15
    WEIGHT_ANOMALIES = 0.15
    WEIGHT_CONCURRENT_LOAD = 0.10

    # Thresholds for scoring (in consistent units)
    DATA_VOLUME_CRITICAL_GB = 1024      # >1TB = 1.0
    DATA_VOLUME_LOW_MB = 1              # <1MB = 0.0
    LOCK_TIME_CRITICAL_MS = 10000       # >10 seconds
    LOCK_TIME_HIGH_MS = 1000
    LOCK_TIME_MEDIUM_MS = 100
    DEPENDENCY_COUNT_CRITICAL = 10

    @staticmethod
    def calculate_data_volume_score(table_size_mb: int) -> RiskFactor:
        """
        Calculate risk from data volume

        Formula: linear interpolation
        - <1MB = 0.0 (low risk)
        - 1GB = 0.5 (medium risk)
        - >1TB = 1.0 (critical risk)
        """
        if table_size_mb > 1024 * 1024:  # >1TB
            score = 1.0
        elif table_size_mb < 1:
            score = 0.0
        else:
            # Linear interpolation
            score = table_size_mb / (1024 * 1024)

        return RiskFactor(
            name="data_volume",
            value=min(score, 1.0),
            unit="bytes",
            weight=RiskScoringFormula.WEIGHT_DATA_VOLUME,
            description=f"Table size: {table_size_mb}MB",
        )

    @staticmethod
    def calculate_lock_time_score(estimated_lock_ms: int) -> RiskFactor:
        """
        Calculate risk from lock time

        Formula: exponential scaling
        - 100ms = 0.1
        - 1s = 0.5
        - 10s+ = 1.0
        """
        if estimated_lock_ms > 10000:
            score = 1.0
        elif estimated_lock_ms < 100:
            score = estimated_lock_ms / 1000
        else:
            # Log scaling between 100ms and 10s
            score = (
                (
                    __import__("math").log(estimated_lock_ms / 100)
                    / __import__("math").log(100)
                )
                * 0.9
                + 0.1
            )

        return RiskFactor(
            name="lock_time",
            value=min(score, 1.0),
            unit="milliseconds",
            weight=RiskScoringFormula.WEIGHT_LOCK_TIME,
            description=f"Estimated lock time: {estimated_lock_ms}ms",
        )

    @staticmethod
    def calculate_dependency_score(
        foreign_keys: int,
        triggers: int,
        views: int,
    ) -> RiskFactor:
        """
        Calculate risk from dependencies

        Formula: linear in dependency count
        - 0 dependencies = 0.0
        - 10+ dependencies = 1.0
        """
        dependency_count = foreign_keys + triggers + views
        score = min(dependency_count / 10, 1.0)

        return RiskFactor(
            name="dependencies",
            value=score,
            unit="count",
            weight=RiskScoringFormula.WEIGHT_DEPENDENCIES,
            description=f"FKs: {foreign_keys}, Triggers: {triggers}, Views: {views}",
        )

    @staticmethod
    def calculate_anomaly_score(anomalies: list[DataAnomaly]) -> RiskFactor:
        """
        Calculate risk from detected anomalies

        Formula: average severity if anomalies exist
        - CRITICAL anomaly = 1.0
        - HIGH = 0.7
        - MEDIUM = 0.3
        - LOW = 0.1
        """
        if not anomalies:
            score = 0.0
        else:
            severity_scores = [
                1.0 if a.severity == Severity.CRITICAL
                else 0.7 if a.severity == Severity.HIGH
                else 0.3 if a.severity == Severity.MEDIUM
                else 0.1
                for a in anomalies
            ]
            score = sum(severity_scores) / len(severity_scores)

        return RiskFactor(
            name="anomalies",
            value=score,
            unit="count",
            weight=RiskScoringFormula.WEIGHT_ANOMALIES,
            description=f"Anomalies detected: {len(anomalies)}",
        )

    @staticmethod
    def calculate_concurrent_load_score(
        active_connections: int,
        max_connections: int = 100,
    ) -> RiskFactor:
        """
        Calculate risk from concurrent load

        Formula: linear in connection utilization
        - <10% = 0.0
        - 50% = 0.5
        - >90% = 1.0
        """
        utilization = active_connections / max_connections
        score = min(max(utilization - 0.1, 0) / 0.9, 1.0)

        return RiskFactor(
            name="concurrent_load",
            value=score,
            unit="percent",
            weight=RiskScoringFormula.WEIGHT_CONCURRENT_LOAD,
            description=f"Connection utilization: {utilization*100:.1f}%",
        )

    @staticmethod
    def calculate_overall_risk(
        factors: dict[str, RiskFactor],
    ) -> tuple[RiskLevel, float]:
        """
        Calculate overall risk score from factors

        Formula: weighted sum
        overall_score = Î£(factor.value * factor.weight)
        """
        overall_score = sum(
            factor.value * factor.weight for factor in factors.values()
        )

        # Map score to risk level
        if overall_score >= 0.75:
            level = RiskLevel.CRITICAL
        elif overall_score >= 0.50:
            level = RiskLevel.HIGH
        elif overall_score >= 0.25:
            level = RiskLevel.MEDIUM
        else:
            level = RiskLevel.LOW

        return level, overall_score
```

**Benefit**: Formula is explicit, reproducible, auditable.

---

#### 4.2 Downtime Prediction with Confidence Bounds (NEW - CRITICAL)

```python
@dataclass
class DowntimeEstimate:
    """Downtime estimate with explicit uncertainty"""
    estimated_downtime_ms: int             # Point estimate
    lower_bound_ms: int                    # 80% confidence lower
    upper_bound_ms: int                    # 80% confidence upper
    confidence_level: float                # 0.0-1.0
    estimate_method: str                   # "heuristic", "historical"
    contributing_factors: dict[str, int]
    caveats: list[str]                     # When to be skeptical


class DowntimePredictor:
    """Predict migration downtime with confidence"""

    def __init__(self, historical_data: HistoricalMigrations | None = None):
        self.historical_data = historical_data
        self.prediction_method = (
            "historical" if historical_data else "heuristic"
        )

    async def predict_downtime(
        self,
        operation: MigrationOperation,
    ) -> DowntimeEstimate:
        """Predict downtime with confidence intervals"""

        if self.prediction_method == "historical":
            return await self._predict_from_history(operation)
        else:
            return await self._predict_heuristic(operation)

    async def _predict_from_history(
        self,
        operation: MigrationOperation,
    ) -> DowntimeEstimate:
        """Use historical data to predict downtime"""

        # Find similar past migrations
        similar = self.historical_data.find_similar(
            table_size_mb=operation.table_size_mb,
            operation_type=operation.type,
            max_results=10,
        )

        if not similar:
            # Fall back to heuristic
            return await self._predict_heuristic(operation)

        actual_downtimes = [m.actual_downtime_ms for m in similar]
        import statistics

        mean = statistics.mean(actual_downtimes)
        stdev = (
            statistics.stdev(actual_downtimes)
            if len(actual_downtimes) > 1
            else 0
        )

        return DowntimeEstimate(
            estimated_downtime_ms=int(mean),
            lower_bound_ms=max(0, int(mean - 2 * stdev)),
            upper_bound_ms=int(mean + 2 * stdev),
            confidence_level=1.0 - (stdev / mean) if mean > 0 else 0.5,
            estimate_method="historical",
            contributing_factors={
                "similar_migrations": len(similar),
                "average_actual_downtime_ms": int(mean),
                "std_deviation_ms": int(stdev),
            },
            caveats=[
                f"Based on {len(similar)} similar migrations",
                f"Standard deviation: {stdev:.0f}ms",
                f"System load on current date may differ",
                f"Database statistics may have changed",
            ],
        )

    async def _predict_heuristic(
        self,
        operation: MigrationOperation,
    ) -> DowntimeEstimate:
        """Heuristic prediction (no historical data)"""

        # Base times in milliseconds
        base_time_ms = {
            "ADD_COLUMN": 100,
            "DROP_COLUMN": 100,
            "RENAME_COLUMN": 50,
            "ALTER_TYPE": 500,
            "ADD_INDEX": 50,
            "DROP_INDEX": 20,
            "ADD_CONSTRAINT": 200,
            "DROP_CONSTRAINT": 50,
        }.get(operation.type, 100)

        # Adjust by table size (size in GB)
        size_gb = operation.table_size_mb / 1024

        # Different operation types scale differently
        if operation.type == "ALTER_TYPE":
            # Full table rewrite - 2ms per GB
            size_adjustment = int(size_gb * 2000)
        elif operation.type == "ADD_INDEX":
            # Index build - 0.5ms per GB
            size_adjustment = int(size_gb * 500)
        else:
            # Most operations - 1ms per GB
            size_adjustment = int(size_gb * 1000)

        estimated = base_time_ms + size_adjustment

        # High uncertainty for heuristic
        return DowntimeEstimate(
            estimated_downtime_ms=estimated,
            lower_bound_ms=max(0, int(estimated * 0.5)),    # -50%
            upper_bound_ms=int(estimated * 2.0),             # +100%
            confidence_level=0.3,  # Low confidence (heuristic only)
            estimate_method="heuristic",
            contributing_factors={
                "base_time_ms": base_time_ms,
                "size_adjustment_ms": size_adjustment,
                "table_size_mb": operation.table_size_mb,
            },
            caveats=[
                "âš ï¸ HEURISTIC ESTIMATE - Low confidence (0.3/1.0)",
                "No historical data available for calibration",
                "Actual downtime depends on:",
                "  - System load and concurrent queries",
                "  - Database configuration (work_mem, etc.)",
                "  - Lock contention from other operations",
                "  - Hardware capabilities (SSD vs HDD)",
                "RECOMMENDATION: Record actual downtime to improve predictions",
                "Next prediction will be more accurate if historical data collected",
            ],
        )
```

**Benefit**: Honest about uncertainty. Users know when predictions are unreliable.

---

**Risk Assessment Deliverables**:
- âœ… `python/confiture/core/risk/scoring.py` (400 lines)
- âœ… `python/confiture/core/risk/predictor.py` (300 lines)
- âœ… `python/confiture/core/risk/analyzer.py` (250 lines)
- âœ… `python/confiture/core/risk/recommendations.py` (150 lines)
- âœ… Tests: `tests/integration/test_risk_assessment.py` (600 lines)

**Total Risk Assessment**: 1,200 lines (vs 800 in v1.0)

---

### 5. Service Level Objectives & Monitoring (NEW - 300 lines)

```python
from dataclasses import dataclass

@dataclass
class ServiceLevelObjective:
    """Define SLOs for Phase 6 operations"""

    # Hook execution SLOs
    HOOK_EXECUTION_LATENCY_P99_MS = 50  # 50ms per hook max
    HOOK_EXECUTION_LATENCY_P95_MS = 30
    HOOK_EXECUTION_TIMEOUT_MS = 30000   # 30 second global timeout

    # Risk assessment SLOs
    RISK_ASSESSMENT_LATENCY_P99_MS = 5000   # 5 second max
    RISK_ASSESSMENT_LATENCY_P95_MS = 2000

    # Profiling overhead SLOs
    PROFILING_OVERHEAD_PERCENT = 5.0        # Max 5% overhead
    PROFILING_ACCURACY_MINIMUM = 0.8        # At least 80% accurate

    # Rule library SLOs
    RULE_COMPOSITION_LATENCY_MS = 100   # <100ms to compose
    RULE_CONFLICT_DETECTION_LATENCY_MS = 50

    # Performance baseline SLOs
    BASELINE_LOOKUP_LATENCY_MS = 10     # <10ms to lookup
    REGRESSION_CHECK_LATENCY_MS = 20    # <20ms to check


class SLOMonitor:
    """Monitor SLO compliance"""

    def __init__(self):
        self.metrics: list[OperationMetric] = []

    def record_metric(
        self,
        operation: str,
        duration_ms: int,
        slo_target_ms: int,
    ) -> None:
        """Record operation metric"""
        compliant = duration_ms <= slo_target_ms
        self.metrics.append(
            OperationMetric(
                operation=operation,
                duration_ms=duration_ms,
                slo_target_ms=slo_target_ms,
                compliant=compliant,
            )
        )

    def get_slo_compliance(self, operation: str, percentile: int = 95) -> float:
        """Get SLO compliance percentage"""
        metrics = [m for m in self.metrics if m.operation == operation]
        if not metrics:
            return 0.0

        compliant = sum(1 for m in metrics if m.compliant)
        return (compliant / len(metrics)) * 100
```

---

## ðŸ“‹ COMPLETE FILE STRUCTURE

```
python/confiture/core/
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ phases.py (100 lines) - Event categorization
â”‚   â”œâ”€â”€ execution.py (600 lines) - Registry, executor, strategies
â”‚   â”œâ”€â”€ context.py (200 lines) - Type-safe contexts
â”‚   â”œâ”€â”€ observability.py (300 lines) - Tracing, circuit breakers
â”‚   â””â”€â”€ base.py (100 lines) - Hook base class
â”œâ”€â”€ linting/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ versioning.py (150 lines) - Rule versioning
â”‚   â”œâ”€â”€ composer.py (400 lines) - Conflict resolution
â”‚   â”œâ”€â”€ registry.py (150 lines) - Library registry
â”‚   â””â”€â”€ libraries/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ hipaa.py (200 lines)
â”‚       â”œâ”€â”€ sox.py (180 lines)
â”‚       â”œâ”€â”€ gdpr.py (250 lines)
â”‚       â”œâ”€â”€ pci_dss.py (150 lines)
â”‚       â””â”€â”€ general.py (300 lines)
â”œâ”€â”€ performance/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ query_profiler.py (400 lines)
â”‚   â”œâ”€â”€ baseline_manager.py (300 lines)
â”‚   â”œâ”€â”€ recommendations.py (200 lines)
â”‚   â””â”€â”€ storage.py (150 lines)
â””â”€â”€ risk/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ scoring.py (400 lines)
    â”œâ”€â”€ predictor.py (300 lines)
    â”œâ”€â”€ analyzer.py (250 lines)
    â””â”€â”€ recommendations.py (150 lines)

tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_hooks_execution.py (600 lines)
â”‚   â”œâ”€â”€ test_rule_libraries.py (600 lines)
â”‚   â””â”€â”€ test_risk_scoring.py (400 lines)
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_hooks_observability.py (400 lines)
â”‚   â”œâ”€â”€ test_performance_profiling.py (500 lines)
â”‚   â”œâ”€â”€ test_baseline_management.py (350 lines)
â”‚   â””â”€â”€ test_risk_assessment.py (600 lines)
â””â”€â”€ e2e/
    â””â”€â”€ test_advanced_workflows.py (300 lines)

docs/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ hooks-advanced.md (300 lines)
â”‚   â”œâ”€â”€ linting-libraries.md (300 lines)
â”‚   â”œâ”€â”€ performance-profiling.md (250 lines)
â”‚   â””â”€â”€ risk-assessment.md (250 lines)
â”œâ”€â”€ guides/
â”‚   â”œâ”€â”€ advanced-hooks-patterns.md (200 lines)
â”‚   â”œâ”€â”€ rule-composition-guide.md (150 lines)
â”‚   â”œâ”€â”€ performance-optimization.md (250 lines)
â”‚   â”œâ”€â”€ risk-assessment-guide.md (200 lines)
â”‚   â””â”€â”€ production-workflows.md (250 lines)
â””â”€â”€ architecture/
    â””â”€â”€ phase-6-architecture.md (400 lines)
```

---

## ðŸ“Š SUMMARY: PHASE 6 REFINED

| Component | v1.0 | v2.0 (Refined) | Improvement |
|-----------|------|---|---|
| Hook System | 1,200 lines | 1,800 lines | +600 (execution model, tracing, type safety) |
| Rule Libraries | 1,500 lines | 2,000 lines | +500 (versioning, conflict resolution) |
| Performance Profiling | 1,000 lines | 1,400 lines | +400 (observable overhead, statistics) |
| Risk Assessment | 800 lines | 1,200 lines | +400 (transparent formula, confidence bounds) |
| SLO & Monitoring | 0 lines | 300 lines | +300 (NEW category) |
| **Implementation** | **5,000 lines** | **6,700 lines** | **+1,700 (34% increase)** |
| **Tests** | 1,200 lines | 1,500 lines | +300 lines |
| **Documentation** | 1,400 lines | 2,000 lines | +600 lines |
| **Total Deliverables** | 8,600 lines | 10,200 lines | +1,600 lines |

---

## âœ… ARCHITECTURAL IMPROVEMENTS CHECKLIST

- âœ… Rec 1: Explicit Hook Execution Model - IMPLEMENTED
- âœ… Rec 2: Type-Safe Hook Contexts - IMPLEMENTED
- âœ… Rec 3: Separate Event Categories - IMPLEMENTED
- âœ… Rec 4: Publish Risk Scoring Formula - IMPLEMENTED
- âœ… Rec 5: Add Confidence Bounds to Predictions - IMPLEMENTED
- âœ… Rec 6: Rule Conflict Resolution - IMPLEMENTED
- âœ… Rec 7: Observability Infrastructure - IMPLEMENTED
- âœ… Rec 8: Define Service Level Objectives - IMPLEMENTED

**Architecture Maturity**: 50% â†’ 90%+ âœ…

---

## ðŸš€ NEXT PHASE: IMPLEMENTATION

With this refined architecture, Phase 6 is ready for:

1. **TDD Development** (RED â†’ GREEN â†’ REFACTOR â†’ QA)
2. **Architecture Review** (validate design decisions)
3. **Implementation Sprints** (1-2 weeks per component)
4. **Quality Gates** (95%+ test coverage, SLO compliance)
5. **Production Deployment** (versioned release, migration guide)

---

**Version**: 2.0 (Architecturally Refined)
**Status**: âœ… READY FOR IMPLEMENTATION
**Date**: January 16, 2026
**Quality Target**: Industrial-Grade Production Ready

